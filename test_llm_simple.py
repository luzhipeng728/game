 #!/usr/bin/env python3
"""ç®€å•LLMæµ‹è¯•"""

import sys
sys.path.append('.')

from sultans_game.config import get_model_config
from langchain_openai import ChatOpenAI

def test_llm():
    """æµ‹è¯•LLMè°ƒç”¨"""
    print("ğŸ”§ æ­£åœ¨åˆ›å»ºLLM...")
    
    try:
        config = get_model_config()
        print(f"ğŸ“‹ ä½¿ç”¨é…ç½®: {config['model']}")
        
        llm = ChatOpenAI(
            model=config["model"],
            temperature=0.7,
            base_url=config["base_url"],
            api_key=config["api_key"],
        )
        
        print("âœ… LLMåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•è°ƒç”¨
        test_prompt = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·ç®€å•å›ç­”ï¼šä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿï¼ˆå›ç­”è¦ç®€æ´ï¼Œä¸è¶…è¿‡50å­—ï¼‰"
        
        print("ğŸ”„ æ­£åœ¨æµ‹è¯•LLMè°ƒç”¨...")
        
        # å°è¯•ä¸åŒçš„è°ƒç”¨æ–¹å¼
        if hasattr(llm, 'invoke'):
            print("ğŸ“ ä½¿ç”¨invokeæ–¹æ³•...")
            result = llm.invoke(test_prompt)
            response = result.content if hasattr(result, 'content') else str(result)
            print(f"âœ… invokeæ–¹æ³•æˆåŠŸ: {response}")
        elif hasattr(llm, '__call__'):
            print("ğŸ“ ä½¿ç”¨__call__æ–¹æ³•...")
            result = llm(test_prompt)
            response = result.content if hasattr(result, 'content') else str(result)
            print(f"âœ… __call__æ–¹æ³•æˆåŠŸ: {response}")
        elif hasattr(llm, 'predict'):
            print("ğŸ“ ä½¿ç”¨predictæ–¹æ³•...")
            response = llm.predict(test_prompt)
            print(f"âœ… predictæ–¹æ³•æˆåŠŸ: {response}")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„è°ƒç”¨æ–¹æ³•")
            print(f"LLMå¯¹è±¡çš„æ–¹æ³•: {[m for m in dir(llm) if not m.startswith('_')]}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_llm()